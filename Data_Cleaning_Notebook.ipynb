{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e41355",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Cleaning Notebook\n",
    "\n",
    "This notebook documents the **step-by-step cleaning process** applied to the `Uncleaned_DS_jobs.csv` dataset to produce the cleaned `Cleaned_DS_Jobs.csv`. The cleaning involves handling duplicates, missing values, feature extraction, and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5566d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load datasets\n",
    "raw_df = pd.read_csv('data/raw/Uncleaned_DS_jobs.csv')\n",
    "cleaned_df = pd.read_csv('data/processed/Cleaned_DS_Jobs.csv')\n",
    "\n",
    "print(\"Raw dataset shape:\", raw_df.shape)\n",
    "print(\"Cleaned dataset shape:\", cleaned_df.shape)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa49e1d7",
   "metadata": {},
   "source": [
    "## Step 1: Inspect Raw Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531afe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df.info()\n",
    "raw_df.describe(include='all').T.head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7f8ee6",
   "metadata": {},
   "source": [
    "## Step 2: Remove Duplicates\n",
    "\n",
    "Duplicates were removed, reducing rows from **672 â†’ 660**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df = raw_df.drop_duplicates()\n",
    "print(\"After removing duplicates:\", raw_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7000e",
   "metadata": {},
   "source": [
    "## Step 3: Handle Missing Values\n",
    "\n",
    "Check missing values per column and decide strategies (drop, fill, or mark as NA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f55d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84682cab",
   "metadata": {},
   "source": [
    "## Step 4: Parse Salary Field\n",
    "\n",
    "Split salary estimates into structured fields: `min_salary`, `max_salary`, `salary_currency`, `salary_period`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd9efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_salary(salary_str):\n",
    "    if pd.isnull(salary_str) or salary_str == '-1':\n",
    "        return pd.Series([np.nan, np.nan, None, None])\n",
    "    \n",
    "    salary_str = salary_str.replace('$', '').replace('K','000')\n",
    "    parts = salary_str.split()\n",
    "    try:\n",
    "        if '-' in parts[0]:\n",
    "            min_s, max_s = parts[0].split('-')\n",
    "            min_s, max_s = int(min_s), int(max_s)\n",
    "        else:\n",
    "            min_s = max_s = int(parts[0])\n",
    "    except:\n",
    "        min_s, max_s = np.nan, np.nan\n",
    "    \n",
    "    currency = '$'\n",
    "    period = parts[-1] if len(parts) > 1 else None\n",
    "    return pd.Series([min_s, max_s, currency, period])\n",
    "\n",
    "raw_df[['min_salary','max_salary','salary_currency','salary_period']] = raw_df['Salary Estimate'].apply(parse_salary)\n",
    "raw_df[['Job Title','Salary Estimate','min_salary','max_salary']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268f1da",
   "metadata": {},
   "source": [
    "## Step 5: Extract Location Fields\n",
    "\n",
    "Split location into `city`, `state`, `country`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df[['city','state']] = raw_df['Location'].str.split(',', n=1, expand=True)\n",
    "raw_df['country'] = 'USA'  # Example assumption; adjust as needed\n",
    "raw_df[['Location','city','state','country']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582014b0",
   "metadata": {},
   "source": [
    "## Step 6: Normalize Job Titles\n",
    "\n",
    "Simplify job titles into categories (e.g., Data Scientist, Data Analyst, ML Engineer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simplify_title(title):\n",
    "    title = title.lower()\n",
    "    if 'data scientist' in title:\n",
    "        return 'data scientist'\n",
    "    elif 'analyst' in title:\n",
    "        return 'data analyst'\n",
    "    elif 'engineer' in title:\n",
    "        return 'ml engineer'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "raw_df['job_simp'] = raw_df['Job Title'].apply(simplify_title)\n",
    "raw_df[['Job Title','job_simp']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01af37",
   "metadata": {},
   "source": [
    "## Step 7: Extract Seniority Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa25ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seniority(title):\n",
    "    title = title.lower()\n",
    "    if 'senior' in title or 'sr' in title:\n",
    "        return 'senior'\n",
    "    elif 'jr' in title or 'junior' in title:\n",
    "        return 'junior'\n",
    "    else:\n",
    "        return 'na'\n",
    "\n",
    "raw_df['seniority'] = raw_df['Job Title'].apply(seniority)\n",
    "raw_df[['Job Title','seniority']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95471283",
   "metadata": {},
   "source": [
    "## Step 8: Save Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7276f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_df.to_csv('data/processed/Cleaned_DS_Jobs_generated.csv', index=False)\n",
    "print(\"Saved cleaned dataset to data/processed/Cleaned_DS_Jobs_generated.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
